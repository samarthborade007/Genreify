{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Music Genre Classification\n","In this notebook I tried to learn the basic concepts of neural networks and use it to classify the music files in dataset. Majorly this notebook can be divided into 3 parts:\n","\n","   1) Using ANN \n","\n","   2) Tackling overfitting with ANN\n","\n","   3) Using CNN\n","\n","\n","Also to read the dataset I have used librosa library which only read files <1Mb and one file is greater than the size giving error due to which I have ignored it. The dataset contains the following genres, the keys being the prediction targets\n","    \n","    0: \"disco\",\n","    1: \"metal\",\n","    2: \"reggae\",\n","    3: \"blues\",\n","    4: \"rock\",\n","    5: \"classical\",\n","    6: \"jazz\",\n","    7: \"hiphop\",\n","    8: \"country\",\n","    9: \"pop\"\n"]},{"cell_type":"code","execution_count":6,"metadata":{"_cell_guid":"01a50213-fe5d-431b-9ade-28c438b3bceb","_uuid":"f32a1031-9332-42b6-8335-dee2b86310a3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import os\n","import librosa\n","import math\n","import json \n","import matplotlib.pyplot as plt\n","import numpy as np"]},{"cell_type":"code","execution_count":7,"metadata":{"_cell_guid":"e1ba8988-65f9-4697-a731-e71b25dd171b","_uuid":"d401d5fb-ea22-4562-bc1b-e882f59016ce","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["dataset_path = r\"../Data/genres_original\"\n","json_path = r\"data.json\"\n","SAMPLE_RATE = 22050\n","DURATION = 30\n","SAMPLES_PER_TRACK = SAMPLE_RATE * DURATION"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":8,"metadata":{"_cell_guid":"f0f0a6db-b7e9-468f-890f-36312fe994e7","_uuid":"c44ee71f-62e4-4100-8e2b-d2a84b01cf8e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def save_mfcc(dataset_path, json_path, n_mfcc=13, n_fft=2048,\n","             hop_length=512, num_segments=5):\n","    # Data storage dictionary\n","    data = {\n","        \"mapping\": [],\n","        \"mfcc\": [],\n","        \"labels\": [],\n","    }\n","    samples_ps = int(SAMPLES_PER_TRACK/num_segments) # ps = per segment\n","    expected_vects_ps = math.ceil(samples_ps/hop_length)\n","    \n","    # loop through all the genres\n","    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n","        # ensuring not at root\n","        if dirpath is not dataset_path:\n","            # save the semantic label\n","            dirpath_comp = dirpath.split(\"/\")\n","            semantic_label = dirpath_comp[-1]\n","            data[\"mapping\"].append(semantic_label)\n","            print(f\"Processing: {semantic_label}\")\n","            \n","            # process files for specific genre\n","            for f in filenames:\n","                if(f==str(\"jazz.00054.wav\")):\n","                    # As librosa only read files <1Mb\n","                    continue\n","                else:\n","                    # load audio file\n","                    file_path = os.path.join(dirpath, f)\n","                    signal,sr = librosa.load(file_path,sr=SAMPLE_RATE)\n","                    for s in range(num_segments):\n","                        start_sample = samples_ps * s\n","                        finish_sample = start_sample + samples_ps\n","\n","                        mfcc = librosa.feature.mfcc(signal[start_sample:finish_sample],\n","                                                    sr = sr,\n","                                                    n_fft = n_fft,\n","                                                    n_mfcc = n_mfcc,\n","                                                    hop_length = hop_length)\n","\n","                        mfcc = mfcc.T\n","\n","                        # store mfcc if it has expected length \n","                        if len(mfcc)==expected_vects_ps:\n","                            data[\"mfcc\"].append(mfcc.tolist())\n","                            data[\"labels\"].append(i-1)\n","                            print(f\"{file_path}, segment: {s+1}\")\n","\n","    with open(json_path,\"w\") as f:\n","        json.dump(data,f,indent=4)"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["from IPython.display import clear_output"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing: genres_original\\blues\n"]},{"ename":"TypeError","evalue":"mfcc() takes 0 positional arguments but 1 positional argument (and 2 keyword-only arguments) were given","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32md:\\C009\\Sem 9\\ML\\Genreify\\Code\\music-genre-classification.ipynb Cell 7\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/C009/Sem%209/ML/Genreify/Code/music-genre-classification.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m save_mfcc(dataset_path,json_path,num_segments\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/C009/Sem%209/ML/Genreify/Code/music-genre-classification.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m clear_output()\n","\u001b[1;32md:\\C009\\Sem 9\\ML\\Genreify\\Code\\music-genre-classification.ipynb Cell 7\u001b[0m line \u001b[0;36msave_mfcc\u001b[1;34m(dataset_path, json_path, n_mfcc, n_fft, hop_length, num_segments)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/C009/Sem%209/ML/Genreify/Code/music-genre-classification.ipynb#W5sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m start_sample \u001b[39m=\u001b[39m samples_ps \u001b[39m*\u001b[39m s\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/C009/Sem%209/ML/Genreify/Code/music-genre-classification.ipynb#W5sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m finish_sample \u001b[39m=\u001b[39m start_sample \u001b[39m+\u001b[39m samples_ps\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/C009/Sem%209/ML/Genreify/Code/music-genre-classification.ipynb#W5sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m mfcc \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39;49mfeature\u001b[39m.\u001b[39;49mmfcc(signal[start_sample:finish_sample],\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/C009/Sem%209/ML/Genreify/Code/music-genre-classification.ipynb#W5sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m                             sr \u001b[39m=\u001b[39;49m sr,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/C009/Sem%209/ML/Genreify/Code/music-genre-classification.ipynb#W5sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m                             n_fft \u001b[39m=\u001b[39;49m n_fft,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/C009/Sem%209/ML/Genreify/Code/music-genre-classification.ipynb#W5sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m                             n_mfcc \u001b[39m=\u001b[39;49m n_mfcc,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/C009/Sem%209/ML/Genreify/Code/music-genre-classification.ipynb#W5sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m                             hop_length \u001b[39m=\u001b[39;49m hop_length)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/C009/Sem%209/ML/Genreify/Code/music-genre-classification.ipynb#W5sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m mfcc \u001b[39m=\u001b[39m mfcc\u001b[39m.\u001b[39mT\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/C009/Sem%209/ML/Genreify/Code/music-genre-classification.ipynb#W5sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# store mfcc if it has expected length \u001b[39;00m\n","\u001b[1;31mTypeError\u001b[0m: mfcc() takes 0 positional arguments but 1 positional argument (and 2 keyword-only arguments) were given"]}],"source":["save_mfcc(dataset_path,json_path,num_segments=10)\n","clear_output()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["filepath = r\"../Data/genres_original/blues/blues.0000\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for i in range(2):\n","    audio, sfreq = librosa.load(filepath+str(i)+\".wav\")\n","    time = np.arange(0, len(audio))/sfreq\n","    plt.plot(time,audio)\n","    plt.xlabel(\"Time\")\n","    plt.ylabel(\"Sound Amplitude\")\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Classifier\n","This part uses the concepts of ANN with keras and sequential layers. I have also done splitting in the ratio 70:30\n","\n","The model is Sequential and architecture only has Flatten and the Dense layers available in keras for the basic ANN representation. As it is naive model we can expect it to be overfit. Info on the layers can be found [here](https://machinelearningknowledge.ai/different-types-of-keras-layers-explained-for-beginners/)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import json\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# load data\n","def load_data(dataset_path):\n","    with open(dataset_path,\"r\") as f:\n","        data = json.load(f)\n","    \n","    # Convert list to numpy arrays\n","    inputs = np.array(data[\"mfcc\"])\n","    targets = np.array(data[\"labels\"])    \n","    \n","    return inputs,targets"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["inputs,targets = load_data(r\"./data.json\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# splitting the data\n","from sklearn.model_selection import train_test_split\n","\n","input_train, input_test, target_train, target_test = train_test_split(inputs, targets, test_size=0.3)\n","print(input_train.shape, target_train.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import *"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = Sequential()\n","\n","model.add(Flatten(input_shape=(inputs.shape[1],inputs.shape[2])))\n","model.add(Dense(512, activation='relu'))\n","model.add(Dense(256, activation='relu'))\n","model.add(Dense(64, activation='relu'))\n","model.add(Dense(10, activation='softmax'))\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tensorflow.keras import optimizers\n","adam = optimizers.Adam(lr=1e-4)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.compile(optimizer=adam,\n","             loss=\"sparse_categorical_crossentropy\",\n","             metrics=[\"accuracy\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.compile(optimizer=adam,\n","             loss=\"sparse_categorical_crossentropy\",\n","             metrics=[\"accuracy\"])\n","\n","hist = model.fit(input_train, target_train,\n","                 validation_data = (input_test,target_test),\n","                 epochs = 50,\n","                 batch_size = 32)\n","clear_output()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def plot_history(hist):\n","    plt.figure(figsize=(20,15))\n","    fig, axs = plt.subplots(2)\n","    # accuracy subplot\n","    axs[0].plot(hist.history[\"accuracy\"], label=\"train accuracy\")\n","    axs[0].plot(hist.history[\"val_accuracy\"], label=\"test accuracy\")    \n","    axs[0].set_ylabel(\"Accuracy\")\n","    axs[0].legend(loc=\"lower right\")\n","    axs[0].set_title(\"Accuracy eval\")\n","    \n","    # Error subplot\n","    axs[1].plot(hist.history[\"loss\"], label=\"train error\")\n","    axs[1].plot(hist.history[\"val_loss\"], label=\"test error\")    \n","    axs[1].set_ylabel(\"Error\")\n","    axs[1].set_xlabel(\"Epoch\")\n","    axs[1].legend(loc=\"upper right\")\n","    axs[1].set_title(\"Error eval\")\n","    \n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plot_history(hist)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_error, test_accuracy = model.evaluate(input_test, target_test, verbose=1)\n","print(f\"Test accuracy: {test_accuracy}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Overfitting\n","This part deals with the overfitting of the previous model. We can deal with it by majorly doing the following process.\n","- Making architecture less complicated \n","- Using augmented data\n","- Early stopping of training\n","- Adding dropout layers\n","- Regularization / Standardization  \n","\n","I have added the dropout layers and kernel_regularizers as compared to previous naive model giving the dropout probability as 30%\n","Kernel_regularizers is one of the 3 type of regularizer used to impose penalties. More info can be found [here](https://medium.com/@robertjohn_15390/regularization-in-tensorflow-using-keras-api-48aba746ae21)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import tensorflow.keras as keras"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Overfitting\n","model = Sequential()\n","\n","model.add(Flatten(input_shape=(inputs.shape[1],inputs.shape[2])))\n","model.add(Dense(512, activation='relu', kernel_regularizer = keras.regularizers.l2(0.001)))\n","model.add(Dropout(0.3))\n","model.add(Dense(256, activation='relu', kernel_regularizer = keras.regularizers.l2(0.003)))\n","model.add(Dropout(0.3))\n","model.add(Dense(64, activation='relu', kernel_regularizer = keras.regularizers.l2(0.01)))\n","model.add(Dropout(0.3))\n","model.add(Dense(32, activation='relu'))\n","model.add(Dense(10, activation='softmax'))\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.compile(optimizer=adam,\n","             loss=\"sparse_categorical_crossentropy\",\n","             metrics=[\"accuracy\"])\n","\n","hist = model.fit(input_train, target_train,\n","                 validation_data = (input_test,target_test),\n","                 epochs = 50,\n","                 batch_size = 32)\n","\n","clear_output()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plot_history(hist)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_error, test_accuracy = model.evaluate(input_test, target_test, verbose=1)\n","print(f\"Test accuracy: {test_accuracy}\")"]},{"cell_type":"markdown","metadata":{},"source":["We can see that the overfitting is greatly reduced but still we are not able to get a good accuracy. Now we will try doing it with Convolutional Neural Network (CNN)"]},{"cell_type":"markdown","metadata":{},"source":["# CNN\n","Using keras layers of Conv2D, MaxPool2D, BatchNormalization.\n","\n","CNN layers takes input primarily in 3D shape, so we again have to prepare the dataset in the form and for that, I have used np.newaxis function which adds a column/layer in the data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def prepare_dataset(test_size, validation_size):\n","    X,y = load_data(r\"./data.json\")\n","    \n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size)\n","    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = validation_size)\n","    X_train = X_train[..., np.newaxis]\n","    X_val = X_val[..., np.newaxis]\n","    X_test = X_test[..., np.newaxis]\n","\n","    return X_train, X_val, X_test, y_train, y_val, y_test"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X_train, X_val, X_test, y_train, y_val, y_test = prepare_dataset(0.25, 0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["input_shape = (X_train.shape[1],X_train.shape[2],X_train.shape[3])\n","print(input_shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = Sequential()\n","model.add(Conv2D(64, (3, 3), activation = \"relu\", input_shape = input_shape))\n","model.add(MaxPool2D((3, 3), strides=(2, 2), padding=\"same\"))\n","model.add(BatchNormalization())\n","\n","model.add(Conv2D(32, (3, 3), activation = \"relu\"))\n","model.add(MaxPool2D((3, 3), strides=(2, 2), padding=\"same\"))\n","model.add(BatchNormalization())\n","\n","model.add(Conv2D(32, (2, 2), activation = \"relu\"))\n","model.add(MaxPool2D((2, 2), strides=(2, 2), padding=\"same\"))\n","model.add(BatchNormalization())\n","\n","model.add(Conv2D(16, (1, 1), activation = \"relu\"))\n","model.add(MaxPool2D((1, 1), strides=(2, 2), padding=\"same\"))\n","model.add(BatchNormalization())\n","\n","model.add(Flatten())\n","model.add(Dense(64, activation=\"relu\"))\n","model.add(Dropout(0.3))\n","model.add(Dense(10, activation=\"softmax\"))\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.compile(optimizer=adam,\n","              loss=\"sparse_categorical_crossentropy\",\n","              metrics=[\"accuracy\"])\n","\n","hist = model.fit(X_train, y_train,\n","                 validation_data = (X_val, y_val),\n","                 epochs = 40,\n","                 batch_size = 32)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plot_history(hist)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_error, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n","print(f\"Test accuracy: {test_accuracy}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def predict(model, X, y):\n","    X = X[np.newaxis,...]\n","    prediction = model.predict(X)\n","    predicted_index = np.argmax(prediction, axis=1)\n","    print(f\"Expected index: {y}, Predicted index: {predicted_index}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predict(model, X_test[10], y_test[10])"]},{"cell_type":"markdown","metadata":{},"source":["As we can see the accuracy has improved by Significant amount but still the accuracy is not enough, in the future of this notebook I am planning to implement RNN model in this and finally use the ensembling to get a push in accuracy. However any other suggestions are always invited!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":4}
